# 📖 RAG Q&A System with Azure OpenAI, LangChain & Azure Functions

## 🌟 Project Overview
In this project, I built a **Retrieval-Augmented Generation (RAG) Question-Answering System** that allows users to ask any natural language question and receive precise answers generated by **Azure OpenAI GPT models**.  

The idea is simple but powerful:  
- Documents are processed into **chunks**.  
- Each chunk is converted into **vector embeddings** using Azure OpenAI.  
- These embeddings are stored in **Azure Cognitive Search** (vector database).  
- When a user asks a question, the system retrieves the most relevant chunks and passes them to GPT, which generates a **context-aware answer**.  
- The solution was first tested locally with **FastAPI**, then deployed to production with **Azure Functions** for serverless scalability.  

This simulates how modern AI-powered applications retrieve and reason over private data, rather than relying only on the model’s memory.

---

## 🏗️ End-to-End Architecture

```mermaid
flowchart TD
    A[Raw Documents] --> B[Chunking & Preprocessing]
    B --> C[Azure OpenAI Embeddings]
    C --> D[Azure Cognitive Search Index]
    E[User Query] --> F[FastAPI / Azure Function Endpoint]
    F --> G[Retriever (LangChain and Azure Search)]
    G --> H[Azure OpenAI Chat Model]
    H --> I[Final Answer]
    D --> G

```

