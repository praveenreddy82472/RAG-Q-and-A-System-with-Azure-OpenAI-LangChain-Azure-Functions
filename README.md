# 📖 RAG Q&A System with Azure OpenAI, LangChain & Azure Functions

## 🌟 Project Overview
In this project, I built a **Retrieval-Augmented Generation (RAG) Question-Answering System** that allows users to ask any natural language question and receive precise answers generated by **Azure OpenAI GPT models**.  

The idea is simple but powerful:  
- Documents are processed into **chunks**.  
- Each chunk is converted into **vector embeddings** using Azure OpenAI.  
- These embeddings are stored in **Azure Cognitive Search** (vector database).  
- When a user asks a question, the system retrieves the most relevant chunks and passes them to GPT, which generates a **context-aware answer**.  
- The solution was first tested locally with **FastAPI**, then deployed to production with **Azure Functions** for serverless scalability.  

This simulates how modern AI-powered applications retrieve and reason over private data, rather than relying only on the model’s memory.

---

## 🏗️ End-to-End Architecture

```mermaid
flowchart TD
    A[Raw Documents] --> B[Chunking & Preprocessing]
    B --> C[Azure OpenAI Embeddings]
    C --> D[Azure Cognitive Search Index]
    E[User Query] --> F[FastAPI / Azure Function Endpoint]
    F --> G[Retriever: LangChain + Azure Search]
    G --> H[Azure OpenAI Chat Model]
    H --> I[Final Answer]
    D --> G

```


llm_development:
  langchain:
    approach: "Code-driven pipeline: data preprocessing → embeddings → vector storage → retrieval → LLM"
    pros:
      - "Maximum flexibility (custom prompts, embeddings, multi-step logic)"
      - "Portable across Azure, GCP, AWS, or on-prem"
    cons:
      - "Slower development, requires coding and API management"
      - "Manual scaling and CI/CD setup"
  azure_ai_no_code:
    approach: "No-code pipeline using Blob Storage, Cognitive Search, and OpenAI"
    pros:
      - "Fast development and deployment"
      - "Built-in scaling, monitoring, and security"
    cons:
      - "Limited flexibility and custom logic"
      - "Partial vendor lock-in"
summary:
  langchain: "Flexibility + Portability"
  azure_ai_no_code: "Speed + Simplicity"

